# ğŸ¦ Zulo_Bank: Financial Data Warehouse Project

Zulo_Bank is a stack data engineering and analytics pipeline built for a fictional banking system. It transforms raw banking data into a fully normalized, analytics-ready **Data Warehouse**, structured around a clean **star schema**.

---

## ğŸ“Œ Project Overview

This project delivers a complete banking data model and warehouse setup, including:

âœ… Raw data normalization using 3NF principles  
âœ… Star schema design with fact & dimension tables  
âœ… PostgreSQL database integration  
âœ… CSV-based transformation and staging  

---

## âš™ï¸ Technologies Used

 âº Python (`pandas`, `psycopg2`) â€” ETL and automation  
 âº PostgreSQL â€” relational database storage  
 âº CSV â€” intermediate data staging  
 âº SQL â€” schema definition and querying  

---

## ğŸ”„ Full Workflow Breakdown

### 1. Normalize Raw Banking Tables (3NF)

- Created atomic versions of key entities:
  - `customer`.
  - `account` (linked with `OpeningDate_ID`).
  - `transaction` (linked with `TransactionDate_ID`).
  - `loan` (linked with `StartDate_ID` and `EndDate_ID`).

- Ensured data integrity by breaking composite fields and removing redundancies.

---

### 2. Generate a Central Date Dimension

- Created `date_dim` table with:
  - `date_id`.
  - `day`, `month`, `year`.

- Used it to replace raw date fields across other tables, enabling time-based slicing and aggregation.

---

### 3. Build Dimensions and Facts

**ğŸ§± Dimension Tables:**
- `customer_dim` â€” atomic customer attributes.  
- `account_dim` â€” account type and metadata.  
- `transaction_dim` â€” transaction types and triggers.  
- `loan_dim` â€” loan product details.  
- `date_dim` â€” unified time dimension.

**ğŸ“Š Fact Tables:**
- `transaction_fact_table` â€” records transactions with keys.  
- `loan_fact_table` â€” records loan events and balances.

Tables were designed to support aggregation, filtering, and joins across dimensions.

---

### 4. PostgreSQL Database Setup

- Built a reusable database connection function with `psycopg2`.  
- Created tables programmatically if they didnâ€™t exist.  
- Used SQL `INSERT INTO` statements with placeholders (safe value passing).  
- Validated the integrity of references with foreign keys.  

---

## ğŸ“„ Results & Final Verification

âœ… Normalized all tables to 3NF.  
âœ… Populated dimensions and fact tables successfully.  
âœ… Queried PostgreSQL with joins and filters across the star schema.  
âœ… Database ready for live analytics or BI integration.  

---

## ğŸ“ Notes

 âº Used safe SQL statements with input substitution to prevent injection  
 âº Dates were linked using generated `date_id` for clean time slicing  
 âº Database schema supports scalable analytics and fast querying  
 âº Designed with modular functions to support ongoing updates and migrations
 
